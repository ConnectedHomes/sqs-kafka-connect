#!/usr/bin/env bash
#
# This script and start/stop/bounce a Spark/Zookeeper/Kafka
# environment suitable for local development.
#
# It is the same environment used by the integration tests,
# but with ports forwards so that services can be accessed
# from outside the containers.
#

set -e

readonly VM=default

function usage {
  echo "Usage $0 [-p project] <up|stop|configure-vbox>"
  echo "  project - Docker project name. By default the base directory name"
  exit 1
}

function run {
    local IT_CONFIG=$(dirname $0)/docker-compose.yml
    local STANDALONE_CONFIG=$(dirname $0)/docker-compose-standalone.yml
    docker-compose -f $IT_CONFIG -f $STANDALONE_CONFIG $@
}

function bounce_job {
    run stop job
    run build job
#    run rm -v -f job
    run up -d job
}

function docker_up {
    run up -d spark_master spark_worker kafka zookeeper schema_registry kafka_manager
    sleep 3
    bounce_job
    echo "NOTE: Spark jobs can only be submitted with spark-submit run _inside_ the container."
    # see https://issues.apache.org/jira/browse/SPARK-4389
}

function docker_stop {
    run down -v
}

while getopts ":p:" opt; do
  case $opt in
    p)
      # Override default docker project name
      # so it doesn't need to be specified with -p
      export COMPOSE_PROJECT_NAME=$OPTARG
      export DOCKER_PROJECT=$OPTARG
      ;;
    *)
      usage
      ;;
  esac
done

shift $(($OPTIND - 1))

source $(dirname $0)/env.sh

case $1 in
  up)
    docker_up
    ;;
  down)
    docker_stop
    ;;
  ps)
    run ps
    ;;
  bounce)
    docker_stop
    docker_up
    ;;
  bounce-kafka)
    # down
    run stop kafka zookeeper
    run rm -v -f kafka zookeeper

    # up
    run up -d zookeeper
    sleep 5
    run up -d kafka
    ;;
  bounce-job)
    bounce_job
    ;;
  *)
    usage
    ;;
esac
